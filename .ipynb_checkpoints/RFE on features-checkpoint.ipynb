{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a294a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors3D\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import check_cv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'ALDH1_inhibition' = 1 in y_test: 120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read file\n",
    "original_df = pd.read_csv('tested_molecules_1.csv')\n",
    "\n",
    "# Split the column\n",
    "original_df[['SMILES', 'ALDH1_inhibition']] = original_df['SMILES,\"ALDH1_inhibition\"'].str.split(',', expand=True)\n",
    "original_df.drop('SMILES,\"ALDH1_inhibition\"', axis=1, inplace=True)\n",
    "\n",
    "original_df['ALDH1_inhibition'] = original_df['ALDH1_inhibition'].str.strip('\"')\n",
    "# Read file for original_df_test\n",
    "original_df_test = pd.read_csv('tested_molecules_2.csv')\n",
    "\n",
    "original_df_test = pd.read_csv('tested_molecules_2.csv')\n",
    "original_df_test[['SMILES', 'ALDH1_inhibition']] = original_df_test['SMILES;ALDH1_inhibition'].str.split(';', expand=True)\n",
    "original_df_test.drop('SMILES;ALDH1_inhibition', axis=1, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([original_df, original_df_test], ignore_index=True)\n",
    "\n",
    "X = combined_df.drop('ALDH1_inhibition', axis=1)\n",
    "y = combined_df['ALDH1_inhibition']\n",
    "\n",
    "# Perform train-test split on X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reset the index of X_train, X_test, y_train, y_test\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test_old = X_test.reset_index(drop=True)\n",
    "Y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Check the number of 'ALDH1_inhibition' = 1 in y_test\n",
    "count_aldh1_inhibition_1 = y_test[y_test == '1'].shape[0]\n",
    "print(\"Number of 'ALDH1_inhibition' = 1 in y_test:\", count_aldh1_inhibition_1)\n",
    "\n",
    "all_descriptors = [desc[0] for desc in Descriptors.descList]\n",
    "descriptor_data_list_original = []\n",
    "for i, row in X_train.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
    "    descriptor_values = [getattr(Descriptors, descriptor)(mol) for descriptor in all_descriptors]\n",
    "    descriptor_data_list_original.append(descriptor_values)\n",
    "\n",
    "descriptor_df = pd.DataFrame(descriptor_data_list_original, columns=all_descriptors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dabacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_variables = descriptor_df.copy()\n",
    "corr_matrix = new_df_variables.corr(numeric_only=True).abs()\n",
    "corr_matrix\n",
    "mask = np.triu(np.ones(corr_matrix.shape, dtype=bool), k=1)\n",
    "mask\n",
    "\n",
    "# Select upper triangle of correlation matrix using the boolean mask\n",
    "upper = corr_matrix.where(mask)\n",
    "\n",
    "# Find index of columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop the columns\n",
    "new_df_variables.drop(columns=to_drop, inplace=True)\n",
    "new_df_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = new_df_variables.copy()\n",
    "missing_values = df_selected.isnull().sum().sum()     \n",
    "if missing_values > 0: \n",
    "   print('Remove missing values')\n",
    "else: \n",
    "   print('No missing_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "df_copied = new_df_variables.copy()\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df_copied)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df_copied.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features_with_low_variance(data, threshold):\n",
    "    low_variance_features = []\n",
    "    \n",
    "    for feature in data.columns:\n",
    "        var = data[feature].var()\n",
    "        \n",
    "        if var < threshold:\n",
    "            low_variance_features.append(feature)\n",
    "    \n",
    "    # Remove the features with low variance\n",
    "    data = data.drop(low_variance_features, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = remove_features_with_low_variance(df_scaled, threshold=0.005)\n",
    "\n",
    "#data_filtered = df_scaled_filtered.copy() \n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had no clue how to remove points, based on statistic, so just used chatGPT to get the data with lower amount of \n",
    "# than 50 outliers\n",
    "def remove_features_with_outliers(data, threshold):\n",
    "    outlier_count = []\n",
    "    features_to_remove = []\n",
    "    \n",
    "    for feature in data.columns:\n",
    "        # Calculate the IQR (Interquartile Range) for the current feature\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define the lower and upper bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count the number of outliers\n",
    "        outliers_count = ((data[feature] < lower_bound) | (data[feature] > upper_bound)).sum()\n",
    "        \n",
    "        # Append the feature and its outlier count to the list\n",
    "        outlier_count.append((feature, outliers_count))\n",
    "        \n",
    "        # Check if the feature has more outliers than the threshold\n",
    "        if outliers_count > threshold:\n",
    "            features_to_remove.append(feature)\n",
    "    \n",
    "    # Sort the features based on the outlier count (from lowest to highest)\n",
    "    outlier_count.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Remove the features with more outliers than the threshold\n",
    "    data = data.drop(features_to_remove, axis=1)\n",
    "    \n",
    "    return data\n",
    "data_filtered = remove_features_with_outliers(data_filtered, 90)\n",
    "data_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean for each column\n",
    "mean = data_filtered.mean()\n",
    "\n",
    "# Calculate the standard deviation for each column\n",
    "std_dev = data_filtered.std()\n",
    "\n",
    "# Iterate over each column in the dataframe\n",
    "outlier_features = []\n",
    "for column in data_filtered.columns:\n",
    "    # Calculate the absolute difference between values and the mean\n",
    "    diff = abs(data_filtered[column] - mean[column])\n",
    "    \n",
    "    # Check if the difference is more than four standard deviations\n",
    "    if (diff > 8 * std_dev[column]).any():\n",
    "        outlier_features.append(column)\n",
    "\n",
    "# Drop the outlier features from the dataframe\n",
    "data_filtered = data_filtered.drop(columns=outlier_features)\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_filtered\n",
    "selected_descriptors = [desc for desc in all_descriptors if desc in data_filtered.columns]\n",
    "descriptor_data_list = []\n",
    "for i, row in X_test.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
    "    descriptor_values = [getattr(Descriptors, descriptor)(mol) for descriptor in selected_descriptors]\n",
    "    descriptor_data_list.append(descriptor_values)\n",
    "\n",
    "descriptor_df_test = pd.DataFrame(descriptor_data_list, columns=selected_descriptors)\n",
    "X_test_all_columns_scaled = descriptor_df_test.copy()\n",
    "scaler = MinMaxScaler()\n",
    "X_test_new = scaler.fit_transform(X_test_all_columns_scaled)\n",
    "X_test_new = pd.DataFrame(X_test, columns=X_test_all_columns_scaled.columns)\n",
    "\n",
    "boosting_estimator = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create RFECV with the AdaBoost classifier as the estimator\n",
    "rfecv = RFECV(estimator=boosting_estimator, cv=5, step=1, scoring='accuracy')\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(X_train, Y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[rfecv.support_]\n",
    "\n",
    "print('Optimal number of features:', rfecv.n_features_)\n",
    "print('Best features:', selected_features)\n",
    "\n",
    "# Fit AdaBoost on the training data\n",
    "boosting_estimator.fit(X_train, Y_train)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = boosting_estimator.feature_importances_\n",
    "\n",
    "# Plot feature importance\n",
    "n_features = X_train.shape[1]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.barh(range(n_features), feature_importances, align='center')\n",
    "plt.yticks(range(n_features), X_train.columns.values)\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_features = data_filtered.loc[:, selected_features]\n",
    "df_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbe43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create principal components\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df_selected_features)\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(df_pca.shape[1])]\n",
    "df_pca_converted = pd.DataFrame(df_pca, columns=component_names)\n",
    "\n",
    "df_pca_converted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19abb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(evr*100)\n",
    "    \n",
    "# Cumaltive Variance\n",
    "cv = np.cumsum(evr)\n",
    "print(cv)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dd092",
   "metadata": {},
   "source": [
    "Need 5 principle components for 60% explained and 13 for 90% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "n = pca.n_components_\n",
    "grid = np.arange(1, n + 1)\n",
    "\n",
    "# Explained variance\n",
    "axs[0].bar(grid, evr)\n",
    "axs[0].set(\n",
    "    xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "# Cumulative Variance\n",
    "axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "axs[1].plot([0, n], [0.6, 0.6], color='k', linestyle='-', linewidth=2)\n",
    "axs[1].plot([0, n], [0.9, 0.9], color='k', linestyle='-', linewidth=2)\n",
    "axs[1].set(\n",
    "    xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    ")\n",
    "        \n",
    "# Set up figure\n",
    "fig.set(figwidth=8, dpi=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(\n",
    "   pca.components_.T,                     # transpose the pca matrix \n",
    "   columns=component_names,               # so the columns are the principal components\n",
    "   index=df_selected_features.columns,                      # and the rows are the original features\n",
    ")\n",
    "loadings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f55a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loadings(PC_1, PC_2):\n",
    "    labels = loadings.index\n",
    "    sns.set_style('white')\n",
    "    sns.scatterplot(data=loadings, x=PC_1, y=PC_2, hue=labels, palette = 'Paired')\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='dotted')    \n",
    "    plt.axvline(x=0, color='gray', linestyle='dotted')\n",
    "    plt.axline((-0.2, -0.2), slope = 1,color ='r', linestyle = 'dotted')\n",
    "\n",
    "    plt.legend(ncol =5, title = 'Variables', loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loadings(PC_1 ='PC1', PC_2= 'PC2')               # IS ZO NIETS ZICHTBAAR< Worden te veel variabelen meegenomen. \n",
    "\n",
    "plt.xlabel('Loadings on PC1 (EV = 45.01 %)')\n",
    "plt.ylabel('Loadings on PC2 (EV = 21.57 %)')\n",
    "plt.title('Loadings principal components 1 and 2', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bab61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(label, PC_1, PC_2, X_train, y_train):\n",
    "    labels = y_train.values\n",
    "    sns.set_style('white')\n",
    "    sns.scatterplot(x=df_pca[:, PC_1], y=df_pca[:, PC_2], hue=labels, palette='bright')\n",
    "\n",
    "    plt.axhline(y=0, color='gray', linestyle='dotted')    \n",
    "    plt.axvline(x=0, color='gray', linestyle='dotted')\n",
    "\n",
    "    plt.legend(loc='best', ncol=2, title=label)\n",
    "\n",
    "# Call the function with the appropriate arguments\n",
    "plot_scores(label='ALDH1_inhibition', PC_1=0, PC_2=1, X_train=X_train, y_train=y_train)\n",
    "\n",
    "plt.xlabel('Scores on PC1 (EV = 19.79 %)')\n",
    "plt.ylabel('Scores on PC2 (EV = 7.78 %)')\n",
    "plt.title('Scores separated by ALDH1 inhibition', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the appropriate arguments\n",
    "plot_scores(label='ALDH1_inhibition', PC_1=2, PC_2=3, X_train=X_train, y_train=y_train)\n",
    "\n",
    "plt.xlabel('Scores on PC1 (EV = 19.79 %)')\n",
    "plt.ylabel('Scores on PC2 (EV = 7.78 %)')\n",
    "plt.title('Scores separated by ALDH1 inhibition', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84d416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_descriptors = [desc for desc in all_descriptors if desc in df_selected_features.columns]\n",
    "descriptor_data_list_2 = []\n",
    "for i, row in X_test_old.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
    "    descriptor_values = [getattr(Descriptors, descriptor)(mol) for descriptor in selected_descriptors]\n",
    "    descriptor_data_list_2.append(descriptor_values)\n",
    "\n",
    "descriptor_df_2 = pd.DataFrame(descriptor_data_list_2, columns=selected_descriptors)\n",
    "\n",
    "\n",
    "X_test_all_columns_scaled = descriptor_df_2.copy()\n",
    "scaler = MinMaxScaler()\n",
    "X_test = scaler.fit_transform(X_test_all_columns_scaled)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test_all_columns_scaled.columns)\n",
    "X_train = df_pca\n",
    "\n",
    "\n",
    "# Create a Random Forest as the base estimator\n",
    "base_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier with Random Forest as base estimator\n",
    "boosting_estimator = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the boosting classifier on the training data\n",
    "boosting_estimator.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "Y_pred = boosting_estimator.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, Y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31220329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
